{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1ae4fc1-3a50-4fb9-b844-416b058168ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc190fa8-5375-4a85-b3f9-7aad27c776ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import pytorch_lightning as pl\n",
    "from torchinfo import summary\n",
    "\n",
    "from utils import plot_reach, plot_reconstruction_examples, plot_grid_z\n",
    "from lin_ae_model_behavior import LinearVariationalAutoencoder\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23fe37fd-b5f2-437a-8ada-b796bb5e5861",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ROOT = Path('/Volumes/GoogleDrive/My Drive/NMA-22/naturalistic_arm_movements_ecog')\n",
    "PATH_DATA = PATH_ROOT / 'data' / 'behavior_data'\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "DTYPE = torch.float\n",
    "DEVICE = torch.device(\"mps\")\n",
    "\n",
    "DATASET = np.load(PATH_DATA / \"reaches_scales.npy\")\n",
    "DATASET = np.swapaxes(DATASET, 2, 1)[:, :75, :]\n",
    "\n",
    "PATH_ROI = PATH_ROOT / \"data\" / \"Naturalistic reach ECoG tfrs ROI\"\n",
    "METADATA = pd.read_csv(PATH_ROI / \"power-roi-all-patients-metadata.csv\", index_col=0)\n",
    "\n",
    "# check the dataset shape\n",
    "assert DATASET.shape == (5984, 75, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec43519-1a97-46b7-98fb-45ec93b0cefb",
   "metadata": {},
   "source": [
    "### Load trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e6adec9-01f6-43bf-83c6-0626bc8b7b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path = Path.cwd() / 'tb_logs' / 'LinearVariationalAutoencoder_n_latent=4_lr=0.005' / 'version_3' /'checkpoints' / 'last.ckpt'\n",
    "model = LinearVariationalAutoencoder.load_from_checkpoint(chk_path, n_input=150, n_latent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac67fa75-32a0-41f6-829d-98b4360fe13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_test = torch.tensor(np.array([[-1, 0, 0, 0]]), dtype=torch.float)\n",
    "\n",
    "with torch.no_grad():\n",
    "    sample = model.decoder(model.decoder_inp(z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8312e8-c28d-4a3a-94c2-317ec2a22d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "548b8684-09d7-4a94-b598-f8a7a8796df6",
   "metadata": {},
   "source": [
    "## Visualize latent space = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e74c4ab-2761-4c3a-8c53-340fc92a04ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "chk_path_2 = Path.cwd() / 'tb_logs' / 'LinearVariationalAutoencoder_n_latent=2_lr=0.005' / 'version_9' /'checkpoints' / 'last.ckpt'\n",
    "model_2 = LinearVariationalAutoencoder.load_from_checkpoint(chk_path_2, n_input=150, n_latent=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53fe866-418c-43d4-a018-dfb48fa44b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_grid_z(model_2, n_latent=2, z_ids=(0, 1), n_ex=7, max_z=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bcb10d-4b2c-4f64-bdf4-4cce007657cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "494cd4a2-1576-4a97-bcea-8a12f72f8201",
   "metadata": {},
   "source": [
    "### Visualize z-values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb3623f2-133a-4131-9275-931f0fdac468",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0d5771c6d148c0830099ff78f3891c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='z1', max=3.0, min=-3.0, step=0.2), FloatSlider(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import ipywidgets\n",
    "\n",
    "def plot_reaches_widget(z1, z2, z3, z4):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    z_ = torch.tensor(np.array([z1, z2, z3, z4]), \n",
    "                      device='cpu', dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "        sample = model.decoder(model.decoder_inp(z_))\n",
    "\n",
    "    plot_reach(ax, torch.swapaxes(\n",
    "        sample.reshape((2, 75)), 1, 0).unsqueeze(0), 0,\n",
    "               plot_ticks_and_labels=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ipywidgets.interact(\n",
    "    plot_reaches_widget, \n",
    "    z1=ipywidgets.FloatSlider(min=-3, max=3, step=0.2, value=0),\n",
    "    z2=ipywidgets.FloatSlider(min=-3, max=3, step=0.2, value=0),\n",
    "    z3=ipywidgets.FloatSlider(min=-3, max=3, step=0.2, value=0),\n",
    "    z4=ipywidgets.FloatSlider(min=-3, max=3, step=0.2, value=0),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3558b2-a186-4dc1-93d2-5e3a4eb7b6fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1b9bf3ce-5efd-43a0-9dfa-4f70a50c3aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ccb8271d06b4e229cd7f2fd9ed89d35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='z1', max=3.0, min=-3.0, step=0.2), FloatSlider(value…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_reaches_widget(z1, z2):\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "    z_ = torch.tensor(np.array([z1, z2]), \n",
    "                      device='cpu', dtype=torch.float)\n",
    "    with torch.no_grad():\n",
    "        sample = model_2.decoder(model_2.decoder_inp(z_))\n",
    "\n",
    "    plot_reach(ax, torch.swapaxes(\n",
    "        sample.reshape((2, 75)), 1, 0).unsqueeze(0), 0,\n",
    "               plot_ticks_and_labels=True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "ipywidgets.interact(\n",
    "    plot_reaches_widget, \n",
    "    z1=ipywidgets.FloatSlider(min=-3, max=3, step=0.2, value=0),\n",
    "    z2=ipywidgets.FloatSlider(min=-3, max=3, step=0.2, value=0),\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15d23e-18e0-4193-a42e-daea5f3f27b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Neuromatch_2022_EcoG_movements",
   "language": "python",
   "name": "neuromatch_2022_ecog_movements"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
