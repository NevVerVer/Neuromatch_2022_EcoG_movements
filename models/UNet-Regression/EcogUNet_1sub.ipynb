{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4daada51-4028-4f19-b7b5-147efbb9fbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "import pathlib\n",
    "import random\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# science\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "\n",
    "# for neural data\n",
    "import mne\n",
    "import nilearn\n",
    "from scipy import signal, stats\n",
    "\n",
    "# for reading this dataset (paper with neural nets classification movement vs no-movement)\n",
    "#from scipy.io import netcdf\n",
    "#import xarray as xr\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import seaborn as sns\n",
    "\n",
    "from livelossplot import PlotLosses\n",
    "from livelossplot.outputs import MatplotlibPlot\n",
    "\n",
    "# prosto dunul\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#!pip install torchsummary\n",
    "from torchsummary import summary\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#from torchinfo import summary\n",
    "\n",
    "sns.set_context('notebook', font_scale=1.2)\n",
    "sns.set_style('ticks')\n",
    "sns.set_palette('muted')\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acbcdd8-c58c-45e5-a3a1-1cbbc2014439",
   "metadata": {},
   "source": [
    "## Load data (1 patient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6bb4af9-e71c-4391-82f6-b69be104109b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Getting data ...\n",
      "Reading subj_04_day_3_l_epo-tfr.h5...\n",
      "Reading D:\\data_nma2022\\tfr data\\subj_04_day_3_l_epo-tfr.h5 ...\n",
      "Adding metadata with 21 columns\n",
      "Data (day 3) shape: (19, 84, 25, 91) = (n_samples, n_chan, n_freqs, n_times)\n",
      "Reading subj_04_day_4_l_epo-tfr.h5...\n",
      "Reading D:\\data_nma2022\\tfr data\\subj_04_day_4_l_epo-tfr.h5 ...\n",
      "Adding metadata with 21 columns\n",
      "Data (day 4) shape: (42, 84, 25, 91) = (n_samples, n_chan, n_freqs, n_times)\n",
      "Reading subj_04_day_5_l_epo-tfr.h5...\n",
      "Reading D:\\data_nma2022\\tfr data\\subj_04_day_5_l_epo-tfr.h5 ...\n",
      "Adding metadata with 21 columns\n",
      "Data (day 5) shape: (32, 84, 25, 91) = (n_samples, n_chan, n_freqs, n_times)\n",
      "Reading subj_04_day_6_l_epo-tfr.h5...\n",
      "Reading D:\\data_nma2022\\tfr data\\subj_04_day_6_l_epo-tfr.h5 ...\n",
      "Adding metadata with 21 columns\n",
      "Data (day 6) shape: (60, 84, 25, 91) = (n_samples, n_chan, n_freqs, n_times)\n",
      "Reading subj_04_day_7_l_epo-tfr.h5...\n",
      "Reading D:\\data_nma2022\\tfr data\\subj_04_day_7_l_epo-tfr.h5 ...\n",
      "Adding metadata with 21 columns\n",
      "Data (day 7) shape: (115, 84, 25, 91) = (n_samples, n_chan, n_freqs, n_times)\n",
      "Labels, shape =  (268,)\n",
      "Features, shape =  (268, 84, 25, 91)\n"
     ]
    }
   ],
   "source": [
    "data_path = r\"D:\\data_nma2022\\tfr data\"\n",
    "\n",
    "# fname_mdata = \"power-roi-all-patients-metadata.csv\"\n",
    "# fpath_mdata = os.path.join(data_path, fname_mdata)\n",
    "\n",
    "print(\"1. Getting data ...\")\n",
    "labels = []\n",
    "features = []\n",
    "days = []\n",
    "for i_session in [3, 4, 5, 6, 7]:\n",
    "    fname_data = f\"subj_04_day_{i_session}_l_epo-tfr.h5\"\n",
    "    fpath_data = os.path.join(data_path, fname_data)\n",
    "    print(f\"Reading {fname_data}...\")\n",
    "    mne_data = mne.time_frequency.read_tfrs(fpath_data)[0]\n",
    "    \n",
    "    data = mne_data.data\n",
    "    metadata = mne_data.metadata\n",
    "\n",
    "    print(f\"Data (day {i_session}) shape: {data.shape} = (n_samples, n_chan, n_freqs, n_times)\")\n",
    "    \n",
    "    labels.append(metadata['reach_a'].to_numpy())\n",
    "    features.append(data)\n",
    "    days.append(metadata['day'].to_numpy())\n",
    "    \n",
    "    \n",
    "labels, features, days = np.concatenate(labels), np.concatenate(features), np.concatenate(days)\n",
    "\n",
    "print(\"Labels, shape = \", labels.shape)\n",
    "print(\"Features, shape = \", features.shape)\n",
    "# print(\"2. Getting metadata ...\")\n",
    "# print(f\"Reading {fname_mdata}...\")\n",
    "# metadata = pd.read_csv(fpath_mdata, index_col=0)\n",
    "# metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a6acd4-1b63-4897-8bbd-6da59f996b76",
   "metadata": {},
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943617ad-4c64-4b07-a25b-919b6444475d",
   "metadata": {},
   "source": [
    "### Within-subject test-split (leave one day out - LODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c019552a-9ae0-4d64-b6d3-d9732b03ed57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X_train: (226, 84, 25, 91) \n",
      " X_test:  (42, 84, 25, 91) \n",
      " y_train: (226,) \n",
      " y_test:  (42,) \n"
     ]
    }
   ],
   "source": [
    "# days = metadata.day.values\n",
    "# labels = metadata.reach_a.values\n",
    "\n",
    "test_day = 4\n",
    "\n",
    "mask_train = (days != test_day)\n",
    "mask_test  = (days == test_day)\n",
    "\n",
    "X_train, y_train = features[mask_train], np.array(labels[mask_train])\n",
    "X_test,  y_test  = features[mask_test],  np.array(labels[mask_test])\n",
    "\n",
    "print(f\" X_train: {X_train.shape} \\n X_test:  {X_test.shape} \\n y_train: {y_train.shape} \\n y_test:  {y_test.shape} \")\n",
    "\n",
    "# creating torch tensors (batch size is variable, so we will create data loader in run_nn func)\n",
    "\n",
    "X_train_lodo = torch.tensor(X_train).float()\n",
    "X_test_lodo = torch.tensor(X_test).float()\n",
    "\n",
    "y_train_lodo = torch.tensor(y_train).float()\n",
    "y_test_lodo = torch.tensor(y_test).float()\n",
    "\n",
    "n_channels, n_freqs, n_times = X_train[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250ea233-fa5c-4806-be33-a779b69df869",
   "metadata": {},
   "source": [
    "## Model building blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89753062-46b3-4a54-8d65-c425210a04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
    "        super().__init__()\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(mid_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, out_channels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.maxpool_conv(x)\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    \"\"\"Upscaling then double conv\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
    "        super().__init__()\n",
    "\n",
    "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
    "        if bilinear:\n",
    "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
    "        else:\n",
    "            self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)\n",
    "            self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # input is CHW\n",
    "        diffY = x2.size()[2] - x1.size()[2]\n",
    "        diffX = x2.size()[3] - x1.size()[3]\n",
    "\n",
    "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                        diffY // 2, diffY - diffY // 2])\n",
    "        # if you have padding issues, see\n",
    "        # https://github.com/HaiyongJiang/U-Net-Pytorch-Unstructured-Buggy/commit/0e854509c2cea854e247a9c615f175f76fbb2e3a\n",
    "        # https://github.com/xiaopeng-liao/Pytorch-UNet/commit/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class OutConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(OutConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28eac41f-0f0b-4ae1-95b9-4c3f434a0b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EcogUNet(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self, n_channels, n_freqs, n_times, args, target_type='sin-cos', bilinear=False):\n",
    "        \"\"\"\n",
    "        UNet-based regression model. \n",
    "        input: \n",
    "            torch.tensor, shape = (batch_size, n_chan, n_freqs, n_times)\n",
    "            target = 'angle' or 'sin'\n",
    "        Depending on the target type the model architecture and loss-function are re-adjusted\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"1 - Initialize parameters\"\"\"\n",
    "        super(EcogUNet, self).__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.n_freqs = n_freqs\n",
    "        self.n_times = n_times\n",
    "        self.bilinear = bilinear\n",
    "        self.target_type = target_type\n",
    "        self.args = args\n",
    "        \n",
    "        self.writer = SummaryWriter()\n",
    "        \n",
    "        \"\"\"2 - Model layers\"\"\"\n",
    "\n",
    "        self.inc = DoubleConv(n_channels, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.down3 = Down(256, 512)\n",
    "        \n",
    "        factor = 2 if bilinear else 1\n",
    "\n",
    "        self.up2 = Up(512, 256 // factor, bilinear)\n",
    "        self.up3 = Up(256, 128 // factor, bilinear)\n",
    "        self.up4 = Up(128, 64, bilinear)  \n",
    "        # - at this point we have vector of shape (batch_size, 64, n_freqs, n_times)\n",
    "        # - reduce the shape of input by // 2 and channels by 2\n",
    "        self.down5 = Down(64, 32)   \n",
    "        self.flatten = nn.Flatten()\n",
    "        \n",
    "        # get shape (batch_size, 32 * n_freqs // 2 * n_times // 2)\n",
    "        # FC1\n",
    "        self.fc1 = nn.Linear(32 * (n_freqs // 2) * (n_times // 2), 512)\n",
    "        self.act1 = nn.SiLU()\n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        # FC2\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.act2 = nn.SiLU()\n",
    "        #self.batchnorm = nn.BatchNorm1d(128)\n",
    "       \n",
    "        \n",
    "        \"\"\"3 - NN-head and loss function\"\"\"\n",
    "        \n",
    "        if target_type == 'angle':\n",
    "      \n",
    "            self.output = nn.Linear(256, 1)\n",
    "            self.out_act = nn.Hardtanh(-np.pi, np.pi)\n",
    "            \n",
    "            self.loss_fn = lambda prediction, label: 1 - torch.cos(prediction, label)\n",
    "            \n",
    "        if target_type == 'sin-cos':\n",
    "    \n",
    "            self.output = nn.Linear(256, 2)\n",
    "            self.out_act = nn.Hardtanh(-1., 1.)\n",
    "            \n",
    "            self.loss_fn = nn.MSELoss(reduction='mean')\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"No such target_type. Use 'angle' or 'sin-cos'.\")\n",
    "            \n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "\n",
    "        x = self.up2(x4, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        # returned to original shape and 64 channels\n",
    "        x = self.down5(x)\n",
    "        x = self.flatten(x)\n",
    "        # FC 1 \n",
    "        x = self.fc1(x)\n",
    "        x = self.act1(x)\n",
    "        x = self.dropout(x)\n",
    "        # FC 2\n",
    "        x = self.fc2(x)\n",
    "        x = self.act2(x)\n",
    "        #x = self.batchnorm(x)\n",
    "        # output layer [-1, 1]\n",
    "        x = self.output(x)\n",
    "        # get angle in rad\n",
    "        out = self.out_act(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        \n",
    "        # convert angle in degrees to desired lable\n",
    "        y = y / 180 * np.pi \n",
    "        if self.target_type == 'sin-cos':\n",
    "            sin = torch.sin(y).unsqueeze(1)\n",
    "            cos = torch.cos(y).unsqueeze(1)\n",
    "            label = torch.cat((sin, cos), dim=1)\n",
    "        if self.target_type == 'angle':\n",
    "            label = y\n",
    "            \n",
    "        # make prediction\n",
    "        prediction = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(prediction, label.view(prediction.shape)).mean()\n",
    "        \n",
    "        # regularization\n",
    "        if self.args['lambda_l1'] != 0:\n",
    "            l1_loss = sum([torch.sum(torch.abs(p)) for p in model.parameters()])\n",
    "            loss = loss + args['lambda_l1'] * l1_loss\n",
    "        if self.args['lambda_l2'] != 0:\n",
    "            l2_loss = sum([torch.sum(p.pow(2)) for p in model.parameters()])\n",
    "            loss = loss + args['lambda_l2'] * l2_loss\n",
    "            \n",
    "        self.log('train_loss', loss)\n",
    "        self.writer.add_scalar('Loss/train', loss.item(), batch_idx)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        # convert angle in degrees to desired lable\n",
    "        y = y / 180 * np.pi \n",
    "        if self.target_type == 'sin-cos':\n",
    "            sin = torch.sin(y).unsqueeze(1)\n",
    "            cos = torch.cos(y).unsqueeze(1)\n",
    "            label = torch.cat((sin, cos), dim=1)\n",
    "        if self.target_type == 'angle':\n",
    "            label = y\n",
    "            \n",
    "        # make prediction\n",
    "        prediction = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(prediction, label.view(prediction.shape)).mean()\n",
    "        self.log('validation_loss', loss)\n",
    "        self.writer.add_scalar('Loss/validation', loss.item(), batch_idx)\n",
    "        return loss\n",
    "    \n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        # convert angle in degrees to desired lable\n",
    "        y = y / 180 * np.pi \n",
    "        if self.target_type == 'sin-cos':\n",
    "            sin = torch.sin(y).unsqueeze(1)\n",
    "            cos = torch.cos(y).unsqueeze(1)\n",
    "            label = torch.cat((sin, cos), dim=1)\n",
    "        if self.target_type == 'angle':\n",
    "            label = y\n",
    "            \n",
    "        # make prediction\n",
    "        prediction = self.forward(x)\n",
    "\n",
    "        loss = self.loss_fn(prediction, label.view(prediction.shape)).mean()\n",
    "        \n",
    "        output = dict({\n",
    "            'test_loss': loss.item()\n",
    "        })\n",
    "        self.writer.add_scalar('Loss/test', loss.item(), batch_idx)\n",
    "        return output\n",
    "    \n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.RMSprop(self.parameters(), lr=self.args['lr'], weight_decay=self.args['weight_decay'], momentum=self.args['momentum'])\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8688918-551d-4d00-8db1-7887b5b0e4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0edb90a4-5b91-4364-802b-42adf154866a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name    | Type       | Params\n",
      "----------------------------------------\n",
      "0  | inc     | DoubleConv | 85.5 K\n",
      "1  | down1   | Down       | 221 K \n",
      "2  | down2   | Down       | 885 K \n",
      "3  | down3   | Down       | 3.5 M \n",
      "4  | up2     | Up         | 2.3 M \n",
      "5  | up3     | Up         | 574 K \n",
      "6  | up4     | Up         | 143 K \n",
      "7  | down5   | Down       | 27.8 K\n",
      "8  | flatten | Flatten    | 0     \n",
      "9  | fc1     | Linear     | 8.8 M \n",
      "10 | act1    | SiLU       | 0     \n",
      "11 | dropout | Dropout    | 0     \n",
      "12 | fc2     | Linear     | 131 K \n",
      "13 | act2    | SiLU       | 0     \n",
      "14 | output  | Linear     | 514   \n",
      "15 | out_act | Hardtanh   | 0     \n",
      "16 | loss_fn | MSELoss    | 0     \n",
      "----------------------------------------\n",
      "16.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "16.8 M    Total params\n",
      "67.017    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467acf3b32eb4c5ab7a3b5f42c5854ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\pytorch_lightning\\trainer\\trainer.py:726: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:495: PossibleUserWarning: Your `test_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test/predict dataloaders.\n",
      "  rank_zero_warn(\n",
      "C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:240: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 12 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b603c983361a452da43bd224f6c05f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@title trian\n",
    "args = {'n_epochs': 100,\n",
    "        'batch_size': 1,\n",
    "        'device': DEVICE, \n",
    "        'lr': 1.5e-3, \n",
    "        'momentum': 0.9, \n",
    "        'weight_decay': 1e-9, \n",
    "        'lambda_l1': 1e-7, \n",
    "        'lambda_l2': 0}\n",
    "\n",
    "model = EcogUNet(n_channels, n_freqs, n_times, args, target_type='sin-cos').double().to(args['device'])\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=args['n_epochs'], accelerator='gpu')\n",
    "\n",
    "train_loader = DataLoader(list(zip(X_train, y_train)), \n",
    "                                   batch_size=args['batch_size'], \n",
    "                                   shuffle=True)\n",
    "\n",
    "test_loader  = DataLoader(list(zip(X_test, y_test)), \n",
    "                                   batch_size=args['batch_size'],\n",
    "                                   shuffle=True)\n",
    "\n",
    "# Perform training\n",
    "trainer.fit(model, train_loader, test_loader)\n",
    "\n",
    "# Perform evaluation\n",
    "trainer.test(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1525a64f-97e5-481f-a561-e321dcee3a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ERROR: Failed to launch TensorBoard (exited with 1).\n",
       "Contents of stderr:\n",
       "Traceback (most recent call last):\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\Scripts\\tensorboard-script.py\", line 5, in <module>\n",
       "    from tensorboard.main import run_main\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\tensorboard\\main.py\", line 27, in <module>\n",
       "    from tensorboard import default\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\tensorboard\\default.py\", line 33, in <module>\n",
       "    from tensorboard.plugins.audio import audio_plugin\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\tensorboard\\plugins\\audio\\audio_plugin.py\", line 23, in <module>\n",
       "    from tensorboard import plugin_util\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\tensorboard\\plugin_util.py\", line 78, in <module>\n",
       "    _MARKDOWN_STORE = _MarkdownStore()\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\tensorboard\\plugin_util.py\", line 70, in __init__\n",
       "    self.markdown = markdown.Markdown(\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\markdown\\core.py\", line 96, in __init__\n",
       "    self.registerExtensions(extensions=kwargs.get('extensions', []),\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\markdown\\core.py\", line 123, in registerExtensions\n",
       "    ext = self.build_extension(ext, configs.get(ext, {}))\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\markdown\\core.py\", line 153, in build_extension\n",
       "    entry_points = [ep for ep in util.get_installed_extensions() if ep.name == ext_name]\n",
       "  File \"C:\\Users\\aleks\\anaconda3\\envs\\compneuro\\lib\\site-packages\\markdown\\util.py\", line 88, in get_installed_extensions\n",
       "    return metadata.entry_points(group='markdown.extensions')\n",
       "TypeError: entry_points() got an unexpected keyword argument 'group'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38396570-be20-4b39-8e1c-7486466a2506",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
